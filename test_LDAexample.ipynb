{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay0</th>\n",
       "      <th>essay1</th>\n",
       "      <th>essay2</th>\n",
       "      <th>essay3</th>\n",
       "      <th>essay4</th>\n",
       "      <th>essay5</th>\n",
       "      <th>essay6</th>\n",
       "      <th>essay7</th>\n",
       "      <th>essay8</th>\n",
       "      <th>essay9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>open-minded, but usually know exactly what i w...</td>\n",
       "      <td>i'm an illustrator by training, and that's alw...</td>\n",
       "      <td>blow jobs, making pretty pictures, bein' sweet...</td>\n",
       "      <td>i asked the friend in closest vicinity just no...</td>\n",
       "      <td>books: fiction - exit to eden, god of small th...</td>\n",
       "      <td>1. my bed&lt;br /&gt;\\n2. tunes&lt;br /&gt;\\n3. good frien...</td>\n",
       "      <td>45% my next move, 50% the current moment, 5% t...</td>\n",
       "      <td>typical, shmypical...</td>\n",
       "      <td>some of my friends call me demanda. but the ri...</td>\n",
       "      <td>you have hair and interesting style (there's m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>update: i'm ready for l o v e !! i just need s...</td>\n",
       "      <td>spending far too much time trying to tactfully...</td>\n",
       "      <td>spelling, listening, learning.</td>\n",
       "      <td>if they met me on the internet first it's usua...</td>\n",
       "      <td>i can give you genres but i'm not a person tha...</td>\n",
       "      <td>god, family &amp;amp; friends, affection, passion,...</td>\n",
       "      <td>everything. i think i have add my mind thinks ...</td>\n",
       "      <td>lately i've been at home *gasp* lazing around ...</td>\n",
       "      <td>i actually watch the mindless entertainment we...</td>\n",
       "      <td>you would like to get to know me better&lt;br /&gt;\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>i've been told i am an extrovert but i feel li...</td>\n",
       "      <td>i just graduated (may 2012) from uc berkeley w...</td>\n",
       "      <td>running, chemistry, soccer, riding horses, rid...</td>\n",
       "      <td>i'd like to say something about a physical att...</td>\n",
       "      <td>books: anything by chuck palahniuk&lt;br /&gt;\\n&lt;br ...</td>\n",
       "      <td>my ability to run, knowledge, chocolate, memor...</td>\n",
       "      <td>all the intermolecular forces going on in ever...</td>\n",
       "      <td>i am usually out with my friends finding the n...</td>\n",
       "      <td>...</td>\n",
       "      <td>-if you are looking to have a fun experience w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>in the city for 11 years, i am an only child f...</td>\n",
       "      <td>i don't know, trying to get by like the rest o...</td>\n",
       "      <td>cooking, sarcasm, snowboarding, dealing with s...</td>\n",
       "      <td>is exactly what i'm thinking (aka my lack of p...</td>\n",
       "      <td>cheese, wine, mexican, homemade or out and eve...</td>\n",
       "      <td>ipod&lt;br /&gt;\\ni would have said cat but mine pas...</td>\n",
       "      <td>finishing my dissertation&lt;br /&gt;\\nmoney&lt;br /&gt;\\n...</td>\n",
       "      <td>taking shots with restaurant friends</td>\n",
       "      <td>censored</td>\n",
       "      <td>if you're interesting and funny&lt;br /&gt;\\nif you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>looking for a good time, life goes by fast not...</td>\n",
       "      <td>meeting new people, traveling and enjoying life</td>\n",
       "      <td>kissing, playing sports, making decisions, tak...</td>\n",
       "      <td>not sure on that one. you will have to let me ...</td>\n",
       "      <td>favorite book - starwars bounty hunters&lt;br /&gt;\\...</td>\n",
       "      <td>1 my iphone&lt;br /&gt;\\n2 sports&lt;br /&gt;\\n3 friends a...</td>\n",
       "      <td>what to do next, traveling, business</td>\n",
       "      <td>i like to go out have fun, see a movie, go to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>you like what i said and want to have fun</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                essay0  \\\n",
       "244  open-minded, but usually know exactly what i w...   \n",
       "245  update: i'm ready for l o v e !! i just need s...   \n",
       "246  i've been told i am an extrovert but i feel li...   \n",
       "247  in the city for 11 years, i am an only child f...   \n",
       "248  looking for a good time, life goes by fast not...   \n",
       "\n",
       "                                                essay1  \\\n",
       "244  i'm an illustrator by training, and that's alw...   \n",
       "245  spending far too much time trying to tactfully...   \n",
       "246  i just graduated (may 2012) from uc berkeley w...   \n",
       "247  i don't know, trying to get by like the rest o...   \n",
       "248    meeting new people, traveling and enjoying life   \n",
       "\n",
       "                                                essay2  \\\n",
       "244  blow jobs, making pretty pictures, bein' sweet...   \n",
       "245                     spelling, listening, learning.   \n",
       "246  running, chemistry, soccer, riding horses, rid...   \n",
       "247  cooking, sarcasm, snowboarding, dealing with s...   \n",
       "248  kissing, playing sports, making decisions, tak...   \n",
       "\n",
       "                                                essay3  \\\n",
       "244  i asked the friend in closest vicinity just no...   \n",
       "245  if they met me on the internet first it's usua...   \n",
       "246  i'd like to say something about a physical att...   \n",
       "247  is exactly what i'm thinking (aka my lack of p...   \n",
       "248  not sure on that one. you will have to let me ...   \n",
       "\n",
       "                                                essay4  \\\n",
       "244  books: fiction - exit to eden, god of small th...   \n",
       "245  i can give you genres but i'm not a person tha...   \n",
       "246  books: anything by chuck palahniuk<br />\\n<br ...   \n",
       "247  cheese, wine, mexican, homemade or out and eve...   \n",
       "248  favorite book - starwars bounty hunters<br />\\...   \n",
       "\n",
       "                                                essay5  \\\n",
       "244  1. my bed<br />\\n2. tunes<br />\\n3. good frien...   \n",
       "245  god, family &amp; friends, affection, passion,...   \n",
       "246  my ability to run, knowledge, chocolate, memor...   \n",
       "247  ipod<br />\\ni would have said cat but mine pas...   \n",
       "248  1 my iphone<br />\\n2 sports<br />\\n3 friends a...   \n",
       "\n",
       "                                                essay6  \\\n",
       "244  45% my next move, 50% the current moment, 5% t...   \n",
       "245  everything. i think i have add my mind thinks ...   \n",
       "246  all the intermolecular forces going on in ever...   \n",
       "247  finishing my dissertation<br />\\nmoney<br />\\n...   \n",
       "248               what to do next, traveling, business   \n",
       "\n",
       "                                                essay7  \\\n",
       "244                              typical, shmypical...   \n",
       "245  lately i've been at home *gasp* lazing around ...   \n",
       "246  i am usually out with my friends finding the n...   \n",
       "247               taking shots with restaurant friends   \n",
       "248  i like to go out have fun, see a movie, go to ...   \n",
       "\n",
       "                                                essay8  \\\n",
       "244  some of my friends call me demanda. but the ri...   \n",
       "245  i actually watch the mindless entertainment we...   \n",
       "246                                                ...   \n",
       "247                                           censored   \n",
       "248                                                NaN   \n",
       "\n",
       "                                                essay9  \n",
       "244  you have hair and interesting style (there's m...  \n",
       "245  you would like to get to know me better<br />\\...  \n",
       "246  -if you are looking to have a fun experience w...  \n",
       "247  if you're interesting and funny<br />\\nif you ...  \n",
       "248          you like what i said and want to have fun  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import copy as cp\n",
    "from collections import Counter\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import time\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "'''\n",
    "\n",
    "This creates a bag of words matrix of the respondents for specific essay questions\n",
    "\n",
    "'''\n",
    "\n",
    "# Load sample essay data\n",
    "essays = pd.read_csv('profiles_essay_sample.csv')\n",
    "essays.tail()\n",
    "\n",
    "# Load full essay data\n",
    "# essays = pd.read_csv('profiles_essay.csv', low_memory=False)\n",
    "# essays.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(common_texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]\n",
    "\n",
    "# Train the model on the corpus.\n",
    "lda = LdaModel(common_corpus, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0computer\n",
      "1human\n",
      "2interface\n",
      "3response\n",
      "4survey\n",
      "5system\n",
      "6time\n",
      "7user\n",
      "8eps\n",
      "9trees\n",
      "10graph\n",
      "11minors\n",
      "human\n",
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n",
      "[[(0, 1), (1, 1), (2, 1)], [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(2, 1), (5, 1), (7, 1), (8, 1)], [(1, 1), (5, 2), (8, 1)], [(3, 1), (6, 1), (7, 1)], [(9, 1)], [(9, 1), (10, 1)], [(9, 1), (10, 1), (11, 1)], [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "for key, val in common_dictionary.items():\n",
    "    print(str(key) + val)\n",
    "print(common_texts)\n",
    "print(common_corpus) # A different way to represent bag of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9, 1), (10, 1)]\n",
      "[(0, 0.033333372), (1, 0.03333337), (2, 0.033333372), (3, 0.2644671), (4, 0.033333372), (5, 0.03333956), (6, 0.033333372), (7, 0.46885973), (8, 0.033333372), (9, 0.033333372)]\n"
     ]
    }
   ],
   "source": [
    "# Create a new corpus, made of previously unseen documents.\n",
    "other_texts = [\n",
    "    ['computer', 'time', 'graph'],\n",
    "    ['survey', 'response', 'eps'],\n",
    "    ['human', 'system', 'computer'],\n",
    "    ['trees','graph']\n",
    "]\n",
    "other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]\n",
    "unseen_doc = other_corpus[3] # For the first index: computer, time, graph\n",
    "print(unseen_doc)\n",
    "vector = lda[other_corpus[3]]  # get topic probability distribution for a document\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.025000047),\n",
       " (1, 0.025000041),\n",
       " (2, 0.025000047),\n",
       " (3, 0.025000043),\n",
       " (4, 0.025000047),\n",
       " (5, 0.7749874),\n",
       " (6, 0.025000047),\n",
       " (7, 0.025012223),\n",
       " (8, 0.025000047),\n",
       " (9, 0.025000047)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.083*\"9\" + 0.083*\"10\" + 0.083*\"5\" + 0.083*\"7\" + 0.083*\"11\" + 0.083*\"6\" + 0.083*\"1\" + 0.083*\"4\" + 0.083*\"2\" + 0.083*\"0\"')\n",
      "(1, '0.337*\"5\" + 0.228*\"8\" + 0.120*\"7\" + 0.120*\"1\" + 0.120*\"2\" + 0.011*\"9\" + 0.011*\"10\" + 0.011*\"11\" + 0.011*\"0\" + 0.011*\"4\"')\n",
      "(2, '0.083*\"9\" + 0.083*\"10\" + 0.083*\"5\" + 0.083*\"2\" + 0.083*\"11\" + 0.083*\"7\" + 0.083*\"6\" + 0.083*\"1\" + 0.083*\"0\" + 0.083*\"4\"')\n",
      "(3, '0.500*\"9\" + 0.045*\"10\" + 0.045*\"5\" + 0.045*\"7\" + 0.045*\"0\" + 0.045*\"11\" + 0.045*\"4\" + 0.045*\"1\" + 0.045*\"6\" + 0.045*\"2\"')\n",
      "(4, '0.083*\"9\" + 0.083*\"10\" + 0.083*\"7\" + 0.083*\"5\" + 0.083*\"6\" + 0.083*\"2\" + 0.083*\"11\" + 0.083*\"4\" + 0.083*\"1\" + 0.083*\"3\"')\n",
      "(5, '0.172*\"3\" + 0.172*\"7\" + 0.172*\"6\" + 0.090*\"5\" + 0.090*\"0\" + 0.090*\"4\" + 0.090*\"9\" + 0.090*\"10\" + 0.008*\"11\" + 0.008*\"2\"')\n",
      "(6, '0.083*\"9\" + 0.083*\"10\" + 0.083*\"7\" + 0.083*\"2\" + 0.083*\"5\" + 0.083*\"11\" + 0.083*\"4\" + 0.083*\"1\" + 0.083*\"0\" + 0.083*\"6\"')\n",
      "(7, '0.206*\"10\" + 0.206*\"11\" + 0.108*\"9\" + 0.108*\"2\" + 0.108*\"4\" + 0.108*\"1\" + 0.108*\"0\" + 0.010*\"7\" + 0.010*\"5\" + 0.010*\"6\"')\n",
      "(8, '0.083*\"9\" + 0.083*\"10\" + 0.083*\"7\" + 0.083*\"11\" + 0.083*\"1\" + 0.083*\"2\" + 0.083*\"5\" + 0.083*\"4\" + 0.083*\"0\" + 0.083*\"6\"')\n",
      "(9, '0.083*\"9\" + 0.083*\"10\" + 0.083*\"5\" + 0.083*\"11\" + 0.083*\"7\" + 0.083*\"1\" + 0.083*\"2\" + 0.083*\"4\" + 0.083*\"0\" + 0.083*\"6\"')\n",
      "0\n",
      "******************************\n",
      "( trees , 0.08336461 )( graph , 0.083341315 )( system , 0.083337925 )( user , 0.083333254 )( minors , 0.08333183 )( time , 0.0833318 )( human , 0.08333072 )( survey , 0.08333002 )( interface , 0.0833266 )( computer , 0.0833257 )\n",
      "******************************\n",
      "1\n",
      "******************************\n",
      "( system , 0.33694768 )( eps , 0.22825828 )( user , 0.11956308 )( human , 0.119562626 )( interface , 0.11956041 )( trees , 0.010876222 )( graph , 0.010873677 )( minors , 0.010871807 )( computer , 0.01087167 )( survey , 0.010871599 )\n",
      "******************************\n",
      "2\n",
      "******************************\n",
      "( trees , 0.08336119 )( graph , 0.083350904 )( system , 0.08333546 )( interface , 0.083334945 )( minors , 0.08333419 )( user , 0.083332166 )( time , 0.08332901 )( human , 0.083328985 )( computer , 0.08332598 )( survey , 0.083323896 )\n",
      "******************************\n",
      "3\n",
      "******************************\n",
      "( trees , 0.49987522 )( graph , 0.045473676 )( system , 0.04546792 )( user , 0.04546586 )( computer , 0.04546579 )( minors , 0.045465607 )( survey , 0.0454656 )( human , 0.045465555 )( time , 0.04546518 )( interface , 0.04546496 )\n",
      "******************************\n",
      "4\n",
      "******************************\n",
      "( trees , 0.08337159 )( graph , 0.08334752 )( user , 0.08333807 )( system , 0.08333624 )( time , 0.08332916 )( interface , 0.08332822 )( minors , 0.08332778 )( survey , 0.08332778 )( human , 0.08332554 )( response , 0.083324455 )\n",
      "******************************\n",
      "5\n",
      "******************************\n",
      "( response , 0.17213267 )( user , 0.17212942 )( time , 0.17212895 )( system , 0.09016788 )( computer , 0.09016607 )( survey , 0.09016474 )( trees , 0.0901626 )( graph , 0.090153396 )( minors , 0.008199058 )( interface , 0.008198756 )\n",
      "******************************\n",
      "6\n",
      "******************************\n",
      "( trees , 0.083366774 )( graph , 0.0833489 )( user , 0.08333585 )( interface , 0.083332345 )( system , 0.08333082 )( minors , 0.0833298 )( survey , 0.083327845 )( human , 0.08332734 )( computer , 0.083327144 )( time , 0.08332699 )\n",
      "******************************\n",
      "7\n",
      "******************************\n",
      "( graph , 0.20588163 )( minors , 0.2058762 )( trees , 0.10784678 )( interface , 0.10784181 )( survey , 0.10784145 )( human , 0.107840925 )( computer , 0.1078409 )( user , 0.009807048 )( system , 0.009806295 )( time , 0.00980591 )\n",
      "******************************\n",
      "8\n",
      "******************************\n",
      "( trees , 0.08335973 )( graph , 0.08335235 )( user , 0.08333637 )( minors , 0.083331436 )( human , 0.08332992 )( interface , 0.083329864 )( system , 0.08332979 )( survey , 0.08332849 )( computer , 0.08332822 )( time , 0.08332645 )\n",
      "******************************\n",
      "9\n",
      "******************************\n",
      "( trees , 0.083369866 )( graph , 0.08335064 )( system , 0.08333816 )( minors , 0.08333231 )( user , 0.08333121 )( human , 0.083330795 )( interface , 0.08332921 )( survey , 0.08332714 )( computer , 0.08332673 )( time , 0.083322324 )\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "for i in lda.show_topics():\n",
    "    print(i)\n",
    "    \n",
    "# Code from: https://stackoverflow.com/questions/17662916/how-to-print-out-the-full-distribution-of-words-in-an-lda-topic-in-gensim    \n",
    "for words in lda.show_topics(formatted=False,num_words=10):\n",
    "    print(words[0])\n",
    "    print(\"******************************\")\n",
    "    for word_prob in words[1]:\n",
    "        print(\"(\",common_dictionary[int(word_prob[0])],\",\",word_prob[1],\")\",end = \"\")\n",
    "    print(\"\")\n",
    "    print(\"******************************\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
